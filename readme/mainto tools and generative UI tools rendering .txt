


Tools (also known as function calling) are programs that you can provide an LLM to extend its built-in functionality. This can be anything from calling an external API to calling functions within your UI.


For models that support tool calls, assistant messages can contain tool call parts, and tool messages can contain tool result parts. A single assistant message can call multiple tools, and a single tool message can contain multiple tool results.

Tool results can be multi-part and multi-modal, e.g. a text and an image. specify multi-part tool results.

Tools are actions that an LLM can invoke. The results of these actions can be reported back to the LLM to be considered in the next response.

For example, when you ask an LLM for the "weather in London", and there is a weather tool available, it could call a tool with London as the argument. The tool would then fetch the weather data and return it to the LLM. The LLM can then use this information in its response.

A tool is an object that can be called by the model to perform a specific task. You can use tools with generateText and streamText by passing one or more tools to the tools parameter.

A tool consists of three properties:

description: An optional description of the tool that can influence when the tool is picked.
parameters: A Zod schema or a JSON schema that defines the parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.
execute: An optional async function that is called with the arguments from the tool call.

If the LLM decides to use a tool, it will generate a tool call. Tools with an execute function are run automatically when these calls are generated. The results of the tool calls are returned using tool result objects.

You can automatically pass tool results back to the LLM using multi-step calls with streamText and generateText.
Schemas
Schemas are used to define the parameters for tools and to validate the tool calls.

The AI SDK supports both raw JSON schemas (using the jsonSchema function) and Zod schemas (either directly or using the zodSchema function).

Zod is a popular TypeScript schema validation library. You can install it with:


Toolkits
When you work with tools, you typically need a mix of application specific tools and general purpose tools. There are several providers that offer pre-built tools as toolkits that you can use out of the box:

Streaming
Streaming conversational text UIs (like ChatGPT) have gained massive popularity over the past few months. This section explores the benefits and drawbacks of streaming and blocking interfaces.

Large language models (LLMs) are extremely powerful. However, when generating long outputs, they can be very slow compared to the latency you're likely used to. If you try to build a traditional blocking UI, your users might easily find themselves staring at loading spinners for 5, 10, even up to 40s waiting for the entire LLM response to be generated. This can lead to a poor user experience, especially in conversational applications like chatbots. Streaming UIs can help mitigate this issue by displaying parts of the response as they become available.

 the streaming UI is able to start displaying the response much faster than the blocking UI. This is because the blocking UI has to wait for the entire response to be generated before it can display anything, while the streaming UI can display parts of the response as they become available.

While streaming interfaces can greatly enhance user experiences, especially with larger language models, they aren't always necessary or beneficial. If you can achieve your desired functionality using a smaller, faster model without resorting to streaming, this route can often lead to simpler and more manageable development processes.

However, regardless of the speed of your model, the AI SDK is designed to make implementing streaming UIs as simple as possible


Agents
When building AI applications, you often need systems that can understand context and take meaningful actions. When building these systems, the key consideration is finding the right balance between flexibility and control. Let's explore different approaches and patterns for building these systems, with a focus on helping you match capabilities to your needs.

Building Blocks
When building AI systems, you can combine these fundamental components:

Single-Step LLM Generation
The basic building block - one call to an LLM to get a response. Useful for straightforward tasks like classification or text generation.

Tool Usage
Enhanced capabilities through tools (like calculators, APIs, or databases) that the LLM can use to accomplish tasks. Tools provide a controlled way to extend what the LLM can do.

When solving complex problems, an LLM can make multiple tool calls across multiple steps without you explicity specifying the order - for example, looking up information in a database, using that to make calculations, and then storing results. The AI SDK makes this multi-step tool usage straightforward through the maxSteps parameter.

Multi-Agent Systems
Multiple LLMs working together, each specialized for different aspects of a complex task. This enables sophisticated behaviors while keeping individual components focused.

Patterns
These building blocks can be combined with workflow patterns that help manage complexity:

Sequential Processing - Steps executed in order
Parallel Processing - Independent tasks run simultaneously
Evaluation/Feedback Loops - Results checked and improved iteratively
Orchestration - Coordinating multiple components
Routing - Directing work based on context
Choosing Your Approach
The key factors to consider:

Flexibility vs Control - How much freedom does the LLM need vs how tightly must you constrain its actions?
Error Tolerance - What are the consequences of mistakes in your use case?
Cost Considerations - More complex systems typically mean more LLM calls and higher costs
Maintenance - Simpler architectures are easier to debug and modify
Start with the simplest approach that meets your needs. Add complexity only when required by:

Breaking down tasks into clear steps
Adding tools for specific capabilities
Implementing feedback loops for quality control
Introducing multiple agents for complex workflows
Let's look at examples of these patterns in action.

Patterns with Examples
The following patterns, adapted from Anthropic's guide on building effective agents, serve as building blocks that can be combined to create comprehensive workflows. Each pattern addresses specific aspects of task execution, and by combining them thoughtfully, you can build reliable solutions for complex problems.

Sequential Processing (Chains)
The simplest workflow pattern executes steps in a predefined order. Each step's output becomes input for the next step, creating a clear chain of operations. 

Routing
This pattern allows the model to make decisions about which path to take through a workflow based on context and intermediate results. The model acts as an intelligent router, directing the flow of execution between different branches of your workflow. This is particularly useful when handling varied inputs that require different processing approaches. In the example below, the results of the first LLM call change the properties of the second LLM call like model size and system prompt.

Parallel Processing
Some tasks can be broken down into independent subtasks that can be executed simultaneously. This pattern takes advantage of parallel execution to improve efficiency while maintaining the benefits of structured workflows. For example, analyzing multiple documents or processing different aspects of a single input concurrently (like code review).

Orchestrator-Worker
In this pattern, a primary model (orchestrator) coordinates the execution of specialized workers. Each worker is optimized for a specific subtask, while the orchestrator maintains overall context and ensures coherent results. This pattern excels at complex tasks requiring different types of expertise or processing.

Evaluator-Optimizer
This pattern introduces quality control into workflows by having dedicated evaluation steps that assess intermediate results. Based on the evaluation, the workflow can either proceed, retry with adjusted parameters, or take corrective action. This creates more robust workflows capable of self-improvement and error recovery.

Multi-Step Tool Usage
If your use case involves solving problems where the solution path is poorly defined or too complex to map out as a workflow in advance, you may want to provide the LLM with a set of lower-level tools and allow it to break down the task into small pieces that it can solve on its own iteratively, without discrete instructions. To implement this kind of agentic pattern, you need to call an LLM in a loop until a task is complete. 

Structured Answers
When building an agent for tasks like mathematical analysis or report generation, it's often useful to have the agent's final output structured in a consistent format that your application can process. You can use an answer tool and the toolChoice: 'required' setting to force the LLM to answer with a structured output that matches the schema of the answer tool. The answer tool has no execute function, so invoking it will terminate the agent.


Getting notified on each completed step
You can use the onStepFinish callback to get notified on each completed step. It is triggered when a step is finished, i.e. all text deltas, tool calls, and tool results for the step are available.

Accessing all steps
Calling generateText with maxSteps can result in several calls to the LLM (steps). You can access information from all steps by using the steps property of the response.'

Generating Structured Data
While text generation can be useful, your use case will likely call for generating structured data. For example, you might want to extract information from text, classify data, or generate synthetic data.

Many language models are capable of generating structured data, often defined as using "JSON modes" or "tools". However, you need to manually provide schemas and then validate the generated data as LLMs can produce incorrect or incomplete structured data.

You can use both functions with different output strategies, e.g. array, object, or no-schema, and with different generation modes, e.g. auto, tool, or json. You can use Zod schemas, Valibot, or JSON schemas to specify the shape of the data that you want, and the AI model will generate data that conforms to that structure.

Accessing response headers & body
Sometimes you need access to the full response from the model provider, e.g. to access some provider-specific headers or body content.


Stream Object
Given the added complexity of returning structured data, model response time can be unacceptable for your interactive use case. With the   function, you can stream the model's response as it is generated.

Output Strategy
You can use both functions with different output strategies, e.g. array, object, or no-schema.

Object
The default output strategy is object, which returns the generated data as an object. You don't need to specify the output strategy if you want to use the default.

Array
If you want to generate an array of objects, you can set the output strategy to array. When you use the array output strategy, the schema specifies the shape of an array element. With  , you can also stream the generated array elements using elementStream.

Enum
If you want to generate a specific enum value, e.g. for classification tasks, you can set the output strategy to enum and provide a list of possible values in the enum parameter.

No Schema
In some cases, you might not want to use a schema, for example when the data is a dynamic user request. You can use the output setting to set the output format to no-schema in those cases and omit the schema parameter.

Sometimes the model will generate invalid or malformed JSON. You can use the   function to attempt to repair the JSON.

It receives the error, either a JSONParseError or a TypeValidationError, and the text that was generated by the model. You can then attempt to repair the text and return the repaired text.


Tool Calling
As covered under Foundations, tools are objects that can be called by the model to perform a specific task. Core tools contain three elements:

description: An optional description of the tool that can influence when the tool is picked.
parameters: A Zod schema or a JSON schema that defines the parameters. The schema is consumed by the LLM, and also used to validate the LLM tool calls.
execute: An optional async function that is called with the arguments from the tool call. It produces a value of type RESULT (generic type). It is optional because you might want to forward tool calls to the client or to a queue instead of executing them in the same process.

Tool calling is not restricted to only text generation. You can also use it to render user interfaces (Generative UI).


Multi-Step Calls 
With the maxSteps setting, you can enable multi-step calls   generate Text and stream Text. When   is set to a number greater than 1 and the model generates a tool call, the  will trigger a new generation passing in the tool result until there are no further tool calls or the maximum number of tool steps is reached.

To decide what value to set for max Steps, consider the most complex task the call might handle and the number of sequential steps required for completion, rather than just the number of available tools.

You may want the model to generate text after the tool has been executed, either to summarize the tool results in the context of the users query. In many cases, you may also want the model to use multiple tools in a single response. This is where multi-step calls come in.

You can think of multi-step calls in a similar way to a conversation with a human. When you ask a question, if the person does not have the requisite knowledge in their common knowledge (a model's training data), the person may need to look up information (use a tool) before they can provide you with an answer. In the same way, the model may need to call a tool to get the information it needs to answer your question where each generation (tool call or text generation) is a step.

Language models can only handle a limited number of tools at a time, depending on the model. To allow for static typing using a large number of tools and limiting the available tools to the model at the same time, provides the experimental active Tools property.

It is an array of tool names that are currently active. By default, the value is undefined and all tools are active.

Generative user interfaces (generative UI) is the process of allowing a large language model (LLM) to go beyond text and "generate UI". This creates a more engaging and AI-native experience for users.

At the core of generative UI are tools , which are functions you provide to the model to perform specialized tasks like getting the weather in a location. The model can decide when and how to use these tools based on the context of the conversation.

Generative UI is the process of connecting the results of a tool call to a React component. Here's how it works:

You provide the model with a prompt or conversation history, along with a set of tools.
Based on the context, the model may decide to call a tool.
If a tool is called, it will execute and return data.
This data can then be passed to a React component for rendering.
By passing the tool results to React components, you can create a generative UI experience that's more engaging and adaptive to your needs.

Create a Tool
Before enhancing your chat interface with dynamic UI elements, you need to create a tool and corresponding React component. A tool will allow the model to perform a specific action, such as fetching weather information.

Update the API route to include the tool you've defined

Now that you've defined the tool and added it to your stream Text call, let's build a React component to display the tools information it returns.

example of weather 

components/weather.tsx

type WeatherProps = {
  temperature: number;
  weather: string;
  location: string;
};

export const Weather = ({ temperature, weather, location }: WeatherProps) => {
  return (
    <div>
      <h2>Current Weather for {location}</h2>
      <p>Condition: {weather}</p>
      <p>Temperature: {temperature}¬∞C</p>
    </div>
  );
};

This component will display the weather information for a given location. It takes three props: temperature, weather, and location (exactly what the weatherTool returns).

Render the Weather Component
Now that you have your tool and corresponding React component, let's integrate them into your chat interface. You'll render the Weather component when the model calls the weather tool.

To check if the model has called a tool, you can use the toolInvocations property of the message object. This property contains information about any tools that were invoked in that generation including toolCallId, toolName, args, toolState, and result.

its just an example 

'use client';


import { Weather } from '@/components/weather';

export default function Page() {
  const { messages, input, handleInputChange, handleSubmit } = useChat();

  return (
    <div>
      {messages.map(message => (
        <div key={message.id}>
          <div>{message.role === 'user' ? 'User: ' : 'AI: '}</div>
          <div>{message.content}</div>

          <div>
            {message.toolInvocations?.map(toolInvocation => {
              const { toolName, toolCallId, state } = toolInvocation;

              if (state === 'result') {
                if (toolName === 'displayWeather') {
                  const { result } = toolInvocation;
                  return (
                    <div key={toolCallId}>
                      <Weather {...result} />
                    </div>
                  );
                }
              } else {
                return (
                  <div key={toolCallId}>
                    {toolName === 'displayWeather' ? (
                      <div>Loading weather...</div>
                    ) : null}
                  </div>
                );
              }
            })}
          </div>
        </div>
      ))}

      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Type a message..."
        />
        <button type="submit">Send</button>
      </form>
    </div>
  );
}

Expanding Your Generative UI Application
You can enhance your chat application by adding more tools and components, creating a richer and more versatile user experience. Here's how you can expand your application:








some simulation of minato use but dont forget dont do the UI we will do it later 

Okay, let's visualize how the results from these specific tools (Calendar, Email, News, Tasks, Reminders) could be rendered in Minato's UI to create an effective and engaging user experience. The key is to blend the AI's conversational summary with clear, glanceable structured data.

Core UI Principles:

Conversational Lead: Minato's spoken/text response should always provide the main summary and context. The UI elements support this, offering detail without being overwhelming.

Card-Based Design: Using distinct cards for different types of information (Calendar event, Email snippet, News headline, Task, Reminder) makes the information easy to scan and digest.

Actionability (Future): While initially read-only, design with future actions in mind (e.g., buttons to "Open Calendar Event", "Mark Task Done", "Reply to Email", "Mark Reminder Done").

Visual Consistency: Use consistent icons, colors, and layouts for different information types.

Responsiveness: Ensure the layout works well on different screen sizes (mobile focus for commute use case).

Visual UI Mockups/Concepts:

Let's imagine the user asks, "Minato, give me my daily briefing."

Minato's Text/Spoken Response:
"Good morning, {UserName}! Looks like a busy day. You have 3 events starting with 'Team Sync' at 9 AM. You've also got 2 new important emails, one from Alice about 'Project Update'. For tasks, 'Finish report' is due today. And for news, the top headline is about [Headline 1] from BBC News. Finally, a reminder to 'Call pharmacy' is due this afternoon."

Accompanying UI Rendering (Conceptual):

(This would likely be a scrollable area within the chat interface)

üìÖ Calendar - Today [View Full Calendar Button]

(Card 1)

[Icon: Calendar] Team Sync

[Icon: Clock] 9:00 AM - 10:00 AM

[Icon: Location Pin - Optional] Conference Room B / Google Meet Link

[Attendees - Optional] Alice, Bob, Charlie

(Card 2)

[Icon: Calendar] Lunch with Bob

[Icon: Clock] 12:30 PM - 1:30 PM

[Icon: Location Pin - Optional] Cafe Central

(Card 3)

[Icon: Calendar] Project Phoenix Meeting

[Icon: Clock] 3:00 PM - 3:30 PM

[Icon: Video Camera - Optional] Zoom Link

‚úâÔ∏è Email Highlights [Open Gmail Button - Future]

(Card 1)

From: Alice [email protected]

Subject: Project Update - Action Required

Date: 8:15 AM Today

Snippet: "Hi team, just wanted to share the latest updates from the client call..." [Expand/Read More Button - Future]

(Card 2)

From: support@company.com

Subject: Regarding Your Ticket #12345**

Date: Yesterday 5:30 PM

Snippet: "Thank you for contacting support. We have an update regarding your issue..." [Expand/Read More Button - Future]

‚úÖ Tasks Due Today [Open Todoist Button - Future]

(Card 1)

[Checkbox Icon] Finish quarterly report

[Project Icon - Optional] #Work [Priority Icon - Optional] P1

[Mark Done Button - Future]

(Card 2)

[Checkbox Icon] Call dentist for appointment

[Project Icon - Optional] #Personal

[Mark Done Button - Future]

üì∞ Top News Headlines [View More News Button]

(Card 1)

[Image - Optional] (Thumbnail from article)

[Headline 1 Text - Bold]

[Icon: Globe] BBC News [Icon: Clock] 1 hour ago

Summary: Short description from the API... [Read Full Article Button]

(Card 2)

[Image - Optional]

[Headline 2 Text - Bold]

[Icon: Globe] TechCrunch [Icon: Clock] 30 mins ago

Summary: Short description from the API... [Read Full Article Button]

üîî Reminders [Manage Reminders Button - Future]

(Card 1)

[Icon: Bell] Call pharmacy

[Icon: Clock] Due: Today 2:00 PM (in 4 hours)

[Mark Done Button - Future] [Snooze Button - Future]

How structuredData enables this:

The Orchestrator returns the structuredData field in its response.

The UI receives this JSON payload.

Based on the result_type (e.g., 'calendar_events', 'email_headers', 'tasks', 'news_articles', 'reminders') within the structuredData, the UI knows which component/card template to render.

The UI then iterates through the arrays (e.g., structuredData.events, structuredData.emails) and populates the cards with the specific details (summary, start time, subject, from, content, title, sourceName, original_content, trigger_datetime etc.).

Links (url) within the structured data are used for "Open..." or "Read Full Article" buttons.

This approach gives the user a quick conversational summary backed up by clear, organized visual details they can glance at or interact with further (eventually).


and consider 


Visualizing the output of a complex, versatile tool like WolframAlphaTool requires flexibility, as its results can range from a simple number to mathematical formulas, data tables, or even images.

Here's how the UI could handle different WolframAlphaTool outputs, using the structuredData returned by your tool:

Recall the Tool's Output:

Your WolframAlphaTool (using getResult) primarily returns:

result: A plain text string representing the direct answer computed by WolframAlpha (e.g., "55", "1.609 kilometers", "Approximately 384,400 km", "2x").

structuredData: Contains { query: input, result_type: 'calculation_or_fact' | 'other', source_api: 'wolframalpha', data: { query: string, result: string } }.

UI Rendering Strategies:

Simple Text Result (Most Common with getResult):

User: "What is 15 * 7 + 3?"

Minato (Text/Voice): "Okay, 15 times 7 plus 3 is 108."

UI Rendering:

(Simple Card)

[Icon: Calculator/Brain] WolframAlpha Result

Query: 15 * 7 + 3

Answer: 108

[Link: View on WolframAlpha - Optional] (Could construct a web query link)

Unit Conversion Result:

User: "Convert 5 miles to kilometers"

Minato (Text/Voice): "Alright, 5 miles is approximately 8.047 kilometers."

UI Rendering:

(Conversion Card)

[Icon: Ruler/Arrows] Unit Conversion

From: 5 miles

To: 8.047 kilometers

[Link: View Calculation - Optional]

Factual Data Result:

User: "What is the population of France?"

Minato (Text/Voice): "According to WolframAlpha, the estimated population of France is about 65.7 million people (as of [date if provided])."

UI Rendering:

(Fact Card)

[Icon: Globe/Info] WolframAlpha Fact

Query: Population of France

Result: ~65.7 million (Est. [Date if available])

[Link: More details on WolframAlpha - Optional]

Mathematical Formula/Expression Result:

User: "Derivative of x^3"

Minato (Text/Voice): "The derivative of x cubed with respect to x is 3x¬≤."

UI Rendering:

(Math Card)

[Icon: Function/Sigma] Mathematical Result

Input: d/dx (x^3)

Result: 3x¬≤ (UI might need MathJax or similar to render formulas nicely)

[Link: View Steps/Plot - Optional]

"No Direct Answer" / Failure:

User: "Tell me about the history of basket weaving" (Too broad for getResult)

Minato (Text/Voice): "Wolfram Alpha couldn't provide a direct short answer for 'the history of basket weaving'. Would you like me to try a web search for that?"

UI Rendering:

(Info Card)

[Icon: Warning/Info] WolframAlpha

Query: History of basket weaving

Status: No direct answer available from WolframAlpha's computational engine for this query.

[Button: Search Web Instead?]

Handling More Complex WolframAlpha Output (If using different API endpoints):

If you were using WolframAlpha APIs that return structured pods (like the full results page), the UI challenge increases:

Data Tables: Render simple HTML tables.

Images/Plots: Display the images directly using <img> tags (if URLs are provided by the API).

Formulas: Use MathJax or KaTeX library client-side to render LaTeX or MathML if the API provides it.

Pod Structure: You might display results in expandable sections corresponding to the WolframAlpha "pods".

Key UI Considerations for WolframAlphaTool:

Clarity: Clearly label the result as coming from WolframAlpha.

Displaying Math: If you expect math formulas, integrate a rendering library like MathJax.

Simplicity: Since getResult aims for simple answers, the UI card should reflect that ‚Äì don't overcomplicate it unless the result is complex (like a formula).

Fallback/Source Link: Offering a link to view the full query on the WolframAlpha website can be helpful for complex queries where the short answer might lack context.

